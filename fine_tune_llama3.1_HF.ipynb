{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "token = os.getenv(\"HF_API_TOKEN\")  # Replace with your actual token\n",
    "HfFolder.save_token(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing More Dependencies\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    "  )\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id, quantization_config=bnb_config, device_map=\"auto\"\n",
    "  )\n",
    "  model.config.use_cache=False\n",
    "  model.config.pretraining_tp=1\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e97db27f054be9ae16b36091a43c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "from time import perf_counter\n",
    "def generate_response(user_input):\n",
    "  prompt = formatted_prompt(user_input)\n",
    "  inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "  generation_config = GenerationConfig(penalty_alpha=0.6,do_sample = True,\n",
    "      top_k=5,temperature=0.5,repetition_penalty=1.2,\n",
    "      max_new_tokens=60,pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "  start_time = perf_counter()\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "  outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "  theresponse = (tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  output_time = perf_counter() - start_time\n",
    "  print(f\"Time taken for inference: {round(output_time,2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_prompt(question)-> str:\n",
    "    return f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "How do I prevent a phishing email?<|im_end|>\n",
      "<|im_start|>assistant: Preventing phishing emails involves being cautious and vigilant when receiving unsolicited messages, especially those with links or attachments. Here are some tips to help you:\n",
      "\n",
      "1\\. Verify the sender's identity:\n",
      "   Check if the message is from a known source.\n",
      "2\\. Look for spelling mistakes and grammatical errors:\n",
      "\n",
      "Time taken for inference: 3.69 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_response(user_input='How do I prevent a phishing email?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Your few-shot examples\n",
    "dataset = [\n",
    "    {\n",
    "        \"abstract\": \"This study investigates the transport of glucose by SGLT1 in human intestinal cells.\",\n",
    "        \"question\": \"Does this abstract provide laboratory or experimental evidence that substrate X is transported by transporter protein Y?\",\n",
    "        \"confidence_score\": 3,\n",
    "        \"justification\": \"The abstract mentions investigation of glucose transport by SGLT1 but does not provide explicit experimental evidence or outcomes.\"\n",
    "    },\n",
    "    {\n",
    "        \"abstract\": \"The effects of various inhibitors on non-specific diffusion were analyzed.\",\n",
    "        \"question\": \"Does this abstract provide laboratory or experimental evidence that substrate X is transported by transporter protein Y?\",\n",
    "        \"confidence_score\": 0,\n",
    "        \"justification\": \"The abstract focuses solely on non-specific diffusion and the effects of inhibitors, without mentioning substrate transport by a specific protein.\"\n",
    "    },\n",
    "    {\n",
    "        \"abstract\": \"ATP-sensitive potassium (K (ATP) ) channels are multimeric protein complexes...\",\n",
    "        \"question\": \"Does this abstract provide laboratory or experimental evidence that substrate X is transported by transporter protein Y?\",\n",
    "        \"confidence_score\": 4,\n",
    "        \"justification\": \"The abstract identifies transporter proteins and their roles, but does not provide direct evidence of substrate transport.\"\n",
    "    },\n",
    "    {\n",
    "        \"abstract\": \"Membrane transporters that use energy stored in sodium gradients to drive nutrients...\",\n",
    "        \"question\": \"Does this abstract provide laboratory or experimental evidence that substrate X is transported by transporter protein Y?\",\n",
    "        \"confidence_score\": 7,\n",
    "        \"justification\": \"The abstract discusses structural insights and galactose binding to vSGLT, strongly implying substrate transport, but lacks direct experimental transport evidence.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_list(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Abstract: {abstract}\\nQuestion: {question}\" for abstract, question in zip(examples[\"abstract\"], examples[\"question\"])]\n",
    "    outputs = examples[\"justification\"]\n",
    "    return {\"input_ids\": tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"],\n",
    "            \"labels\": tokenizer(outputs, truncation=True, padding=\"max_length\", max_length=512)[\"input_ids\"]}\n",
    "\n",
    "tokenized_dataset = train_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",  # No validation since you're evaluating manually\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=10,  # More epochs due to limited examples\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq, AutoProcessor\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\": \"cpu\"})\n",
    "checkpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "   tokenizer=processor,\n",
    "    model=model,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    processing_class = processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = os.getenv(\"WANDB_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
